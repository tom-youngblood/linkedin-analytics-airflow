name: Daily LinkedIn to HubSpot Sync

on:
  schedule:
    # Run 3x daily at random times between 9 AM and 5 PM UTC on weekdays
    - cron: '0 9 * * 1-5'   # First run at 9 AM
    - cron: '0 12 * * 1-5'  # Second run at 12 PM
    - cron: '0 15 * * 1-5'  # Third run at 3 PM
  workflow_dispatch:

jobs:
  sync:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Create necessary directories
      run: |
        mkdir -p data
        mkdir -p scripts/logs
        # Create empty db if it doesn't exist
        if [ ! -f data/post_scrapes.db ]; then
          touch data/post_scrapes.db
          echo "Created new empty database"
        fi

    - name: Calculate random delay
      id: delay
      run: |
        # Get the hour from the schedule
        SCHEDULE_HOUR=$(echo "${{ github.event.schedule }}" | grep -o '[0-9]\+')
        
        # Calculate random delay between 1-3 hours (3600-10800 seconds)
        RANDOM_DELAY=$((3600 + RANDOM % 7200))
        
        # Calculate maximum delay to stay within 5 PM
        MAX_DELAY=$((17 - SCHEDULE_HOUR))
        MAX_DELAY_SECONDS=$((MAX_DELAY * 3600))
        
        # Use the smaller of the two delays
        if [ $RANDOM_DELAY -gt $MAX_DELAY_SECONDS ]; then
          FINAL_DELAY=$MAX_DELAY_SECONDS
        else
          FINAL_DELAY=$RANDOM_DELAY
        fi
        
        echo "Schedule hour: $SCHEDULE_HOUR"
        echo "Random delay: $RANDOM_DELAY seconds"
        echo "Max delay: $MAX_DELAY_SECONDS seconds"
        echo "Final delay: $FINAL_DELAY seconds"
        echo "delay=$FINAL_DELAY" >> $GITHUB_OUTPUT

    - name: Random delay
      run: |
        echo "Sleeping for ${{ steps.delay.outputs.delay }} seconds..."
        sleep ${{ steps.delay.outputs.delay }}
        
    - name: Download database
      uses: actions/download-artifact@v4
      with:
        name: post_scrapes.db
        path: data/
      continue-on-error: true  # Continue if no previous artifact exists
        
    - name: Pull posts from Google Sheets
      env:
        SERVICE_ACCOUNT_KEY: ${{ secrets.SERVICE_ACCOUNT_KEY }}
      run: |
        cd scripts
        echo "Starting Google Sheets sync..."
        python gs_sql.py
        echo "Google Sheets sync completed"
        
    - name: Scrape LinkedIn posts
      env:
        APIFY_API_KEY: ${{ secrets.APIFY_API_KEY }}
      run: |
        cd scripts
        echo "Starting LinkedIn scraping..."
        python scrape.py
        echo "LinkedIn scraping completed"
        
    - name: Sync with HubSpot
      env:
        APIFY_API_KEY: ${{ secrets.APIFY_API_KEY }}
        HUBSPOT_API_KEY: ${{ secrets.HUBSPOT_API_KEY }}
      run: |
        cd scripts
        echo "Starting HubSpot sync..."
        python sql_hs.py
        echo "HubSpot sync completed"

    - name: Upload database
      uses: actions/upload-artifact@v4
      with:
        name: post_scrapes.db
        path: data/post_scrapes.db
        retention-days: 90  # Keep the artifact for 90 days